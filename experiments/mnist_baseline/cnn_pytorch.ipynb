{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rahuljaisy/foundational-cnn-baseline-mnist-pytorch?scriptVersionId=249396010\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nMNIST Digit Recognizer with PyTorch\n</div>","metadata":{"_uuid":"0cecfe02-f52a-4bac-ae89-ebf83524320b","_cell_guid":"6b5c0236-8507-490b-9ede-f0c5f67a2c50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**Objective:** Classify handwritten digit images (0–9) from the Kaggle “Digit Recognizer” (MNIST) dataset. MNIST is a canonical computer vision benchmark of 28×28 grayscale digit images. It contains 60,000 training and 10,000 test images. The task is to build a convolutional neural network (CNN) from first principles for high accuracy digit classification, with requisite scientific rigor and transparency. We load and preprocess the data, define a CNN with modern modules (BatchNorm, Dropout), train it with a detailed loop, and evaluate it with metrics (accuracy, confusion matrix, per-class accuracy). We will also generate final Kaggle formatted predictions (ImageId,Label) for submission.","metadata":{"_uuid":"fbd6fa80-2cf5-4de5-b17b-ad53e5b74265","_cell_guid":"c99736f3-ae5f-40ef-a03d-25bc86c72b0a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\n Setup and Data Loading\n</div>","metadata":{"_uuid":"2ab60bf6-e641-49c4-a45b-3b09fea0d0cd","_cell_guid":"f6abfb13-5a32-4ff2-af5c-ba7bd60c08ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# Reproducibility: fix random seeds\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# Device configuration: use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"477bdd76-3b08-4184-acab-4b870be76e0d","_cell_guid":"15edad30-0ec6-4a05-8b3f-ea663b2a1cb4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We set fixed random seeds for Python, NumPy, and Torch to ensure reproducible splits and training outcomes.\n\nWe configure the compute device for model training.","metadata":{"_uuid":"b0a01f84-23ea-43c6-bb09-f03af20ee382","_cell_guid":"5cdeaa09-6a2b-488a-8ce6-7e778a32749e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load training and test data from CSV (Kaggle input path)\ntrain_df = pd.read_csv('../input/digit-recognizer/train.csv')\ntest_df = pd.read_csv('../input/digit-recognizer/test.csv')\n\n# Separate labels and pixels\ntrain_labels = train_df['label'].values\ntrain_images = train_df.drop('label', axis=1).values\ntest_images = test_df.values\n\n# Normalize pixel values to [0,1]\n# Original MNIST pixels are 0–255, so dividing by 255 yields [0,1].\ntrain_images = train_images.astype(np.float32) / 255.0\ntest_images = test_images.astype(np.float32) / 255.0\n\n# Reshape to image tensors (N, 28, 28)\ntrain_images = train_images.reshape(-1, 28, 28)\ntest_images = test_images.reshape(-1, 28, 28)\n\nprint(f\"Train images: {train_images.shape}, Train labels: {train_labels.shape}\")\nprint(f\"Test images: {test_images.shape}\")","metadata":{"_uuid":"b6de42f6-7e17-40aa-bb44-34903f8497bd","_cell_guid":"230246db-22b6-4525-b731-b9cc64ef826b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We read train.csv and test.csv (provided by Kaggle). The train file has 785 columns: the first is label, the rest are pixels. Each image is 28×28 = 784 pixels.\n\nWe separate features (pixels) and labels, normalize pixel intensities to [0,1], and reshape to (N,28,28).","metadata":{"_uuid":"19bee65e-c9bb-4fe2-8e0f-d7e32f563337","_cell_guid":"661c5896-8a0c-4185-b36a-eaf9acde087a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create a train/validation split (80% train, 20% validation)\nX_train, X_val, y_train, y_val = train_test_split(\n    train_images, train_labels,\n    test_size=0.2,\n    stratify=train_labels,\n    random_state=seed\n)\nprint(f\"Train split: {X_train.shape}, Validation split: {X_val.shape}\")","metadata":{"_uuid":"36596e13-e92c-4b20-984f-798c53fc0205","_cell_guid":"a9a4b859-487e-4517-acd3-a25b7b224606","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We hold out 20% of the training data for validation. Stratified split ensures each digit class is proportionally represented.","metadata":{"_uuid":"3e481e62-d869-4dfc-8c42-e760e8cd10a3","_cell_guid":"a70fa6c6-7ba6-448f-aae3-8e2db86c0951","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define PyTorch Dataset with optional transforms\nclass MNISTDataset(Dataset):\n    def __init__(self, images, labels=None, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img = self.images[idx]\n        # Convert the numpy image to a PIL image for transforms\n        # (MNIST is grayscale, so use mode 'F')\n        img = transforms.ToPILImage()(img)\n        if self.transform:\n            img = self.transform(img)\n        if self.labels is not None:\n            return img, int(self.labels[idx])\n        else:\n            return img\n\n# Data augmentation and normalization transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(15),  # random rotation ±15°\n    transforms.ToTensor(),           # convert PIL image to PyTorch tensor (C,H,W) in [0,1]\n    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST mean/std\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Create dataset objects\ntrain_dataset = MNISTDataset(X_train, y_train, transform=train_transform)\nval_dataset = MNISTDataset(X_val, y_val, transform=test_transform)\ntest_dataset = MNISTDataset(test_images, labels=None, transform=test_transform)\n\n# DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"_uuid":"ad3b930d-becb-4048-9d39-e17e7501e403","_cell_guid":"d04e37b2-7326-4ecb-be65-f2a64088688b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Transforms:** We define random data augmentations for the training set: random rotation (±15°) to increase robustness. Both train and validation images are normalized with mean≈0.1307 and std≈0.3081 (standard MNIST stats).\n\n**Datasets:** We wrap the NumPy arrays in a custom MNIST Dataset so we can apply transforms on the fly.\n\n**DataLoaders:** These iterate over batches. We shuffle the training data but not the validation or test data.","metadata":{"_uuid":"9045e0a6-cd66-404b-8fa2-81e27544623e","_cell_guid":"1f56fe86-4840-48a1-8eef-b2babc41cef4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Visualize a batch of training images with labels (for sanity check)\nimages, labels = next(iter(train_loader))\nfig, axes = plt.subplots(1,5, figsize=(10,2))\nfor i in range(5):\n    ax = axes[i]\n    img = images[i].squeeze().numpy()\n    ax.imshow(img, cmap='gray')\n    ax.set_title(f\"Label: {labels[i].item()}\")\n    ax.axis('off')\nplt.suptitle(\"Sample training images\")\nplt.show()","metadata":{"_uuid":"f9d6398c-95b7-4448-8db3-88bd56f45a16","_cell_guid":"dc182cf5-aea4-4448-a207-6a2cc0547e04","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sample images:** A batch of training images is plotted (without axes) to verify loading and augmentation. (Plotting code shown for completeness.)","metadata":{"_uuid":"3d7ed481-1b95-47da-9939-8e9e716e943e","_cell_guid":"25dbecb4-0120-447f-9846-914ab34aa94b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nCNN Model Definition\n</div>\n\nWe design a convolutional neural network (CNN) with a fully specified architecture, comprising sequential layers of convolution, batch normalization, activation, pooling, and dropout. The model is constructed with complete architectural transparency, with all components explicitly defined to ensure interpretability and precise control over feature extraction.","metadata":{"_uuid":"6d34fb6a-18db-41ef-b968-1717d40f158b","_cell_guid":"03737730-4a3e-4939-a1b1-30ab31b367f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MNIST_CNN(nn.Module):\n    def __init__(self):\n        super(MNIST_CNN, self).__init__()\n        # Convolutional feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # conv1: 1->32\n            nn.BatchNorm2d(32),                           # BN after conv\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),                              # output 32x14x14\n            \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # conv2: 32->64\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),                              # output 64x7x7\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1), # conv3: 64->128\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),                         # no further pooling; output 128x7x7\n        )\n        \n        # Classifier (fully connected layers)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),                            # dropout regularization\n            nn.Linear(128 * 7 * 7, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 10)                             # final outputs (logits) for 10 classes\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)  # flatten (batch_size, 128*7*7)\n        x = self.classifier(x)\n        return x\n\nmodel = MNIST_CNN().to(device)\nprint(model)","metadata":{"_uuid":"d50743db-0883-4cc3-a20b-800185f1af5c","_cell_guid":"2f1c2053-8d9a-45cf-bbaa-0ee562437c18","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nArchitecture: \n</div>\n\n**Features:** Three convolutional blocks (Conv→BatchNorm→ReLU). MaxPooling after the first two reduces spatial size by 2. Batch normalization (BN) is applied to stabilize and accelerate training by normalizing each batch's features.\n\n**Classifier:** We flatten and feed into linear layers. We insert Dropout (p=0.5) before each hidden fully connected layer to reduce overfitting. Final layer outputs 10 logits (one per digit).\n\n**No pre-trained or high level abstractions:** All layers (Conv2d, BatchNorm2d, Linear, etc.) are explicitly instantiated and configured based on established architectural design principles in deep learning.","metadata":{"_uuid":"0b163f4a-dc5f-442a-9f56-bfac1d6b5ba1","_cell_guid":"51a98597-1914-4d72-b0dd-4ca818c67990","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Loss function and optimizer\n# criterion = nn.CrossEntropyLoss()  # for multi-class classification (old version )\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # <-- adds 0.1 smoothing to soften hard targets\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"_uuid":"5827642e-8dea-4d30-80c8-a422e37ce582","_cell_guid":"b2d9e823-2445-4b2d-a0f0-088712e9edeb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use cross entropy loss (which applies softmax internally) and the Adam optimizer.","metadata":{"_uuid":"b56d4b11-148b-47e8-afd2-1f6069d0a673","_cell_guid":"d6c0072b-5d48-415d-9c0c-6ff121b14381","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nTraining Loop\n</div>\n\nWe train for a fixed number of epochs, tracking loss and accuracy. Each epoch runs a full pass over the training data, then evaluates on validation data.","metadata":{"_uuid":"8895ec32-42cf-4cb4-8172-7e20f2250bdf","_cell_guid":"a9ba8e4b-8d2b-44c1-af85-358bc5456ee0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"num_epochs = 150\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(1, num_epochs+1):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to stabilize training\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n    \n    val_loss /= len(val_loader.dataset)\n    val_acc = correct / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    print(f\"Epoch {epoch}/{num_epochs}: Train Loss = {epoch_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")","metadata":{"_uuid":"60ce0652-46e7-41d9-b7a7-eafc6fc0243b","_cell_guid":"1e05a966-d26b-4875-b977-0b3c8374f069","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Epoch loop:** We accumulate total training loss and compute average. After each epoch, we switch to evaluation mode (model.eval()) to compute validation loss and accuracy (without gradient updates).\n\n**Metrics:** We  tried to record training loss, validation loss, and validation accuracy each epoch. This allows plotting learning curves and detecting overfitting/underfitting.","metadata":{"_uuid":"696eea2c-e9c7-42be-8a94-a3b76276e075","_cell_guid":"d56bd2a6-7b38-4f16-9444-cd7113ca12e8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Plot training and validation curves\nepochs = np.arange(1, num_epochs+1)\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(epochs, train_losses, label='Train Loss')\nplt.plot(epochs, val_losses, label='Val Loss')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\nplt.title('Loss vs. Epoch')\n\nplt.subplot(1,2,2)\nplt.plot(epochs, val_accuracies, label='Val Accuracy')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy')\nplt.ylim(0.0, 1.0); plt.legend()\nplt.title('Validation Accuracy')\nplt.show()","metadata":{"_uuid":"07a1ebc9-7593-4276-a14c-db5970988506","_cell_guid":"17fff342-9b51-47ed-9c0d-a48dfcd093d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Learning Curves:** The plots tries to illustrate how the training loss decreases and how validation accuracy evolves. Ideally, training loss steadily falls while validation accuracy rises until convergence. Significant gaps between training and validation indicate overfitting. \n\n**Evaluation:** Confusion Matrix and Class Accuracy\n\nAfter training, we perform a detailed evaluation on the validation set.","metadata":{"_uuid":"022aabb3-74a2-47e0-8fc8-aac7594d5fac","_cell_guid":"6f3c24f4-b071-4991-8b6a-ca77ab804581","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n        all_preds.append(preds.cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nprint(\"Confusion matrix:\\n\", cm)","metadata":{"_uuid":"dac1b2df-54c8-4de7-a2a3-e8fa01d2a0ad","_cell_guid":"dd50cdfb-001c-4a9d-9885-0a1bc06c8c2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Confusion Matrix:** We use sklearn.metrics.confusion_matrix to show counts of true vs. predicted labels. Each row i (true class) shows how many samples were predicted as each class j. The diagonal entries are correct predictions. Off diagonals generally reveal common misclassifications.","metadata":{"_uuid":"9e2ea2fa-e05a-4432-93be-6f57a5f7d6cf","_cell_guid":"857ac6d2-8744-471f-96e2-39be867db395","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Normalize confusion matrix for visualization (optional)\ncm_norm = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True)\nprint(\"Per-class accuracy:\", np.diag(cm_norm))","metadata":{"_uuid":"df7d9baf-b06b-40b5-ad12-01904eddd2bf","_cell_guid":"6dd47b20-88b1-42c9-bf74-da60a8ca1c32","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Class Accuracy:** We compute per-class accuracy as the diagonal of the normalized confusion matrix (cm_norm[i,i]). This shows how well each digit is recognized. A bar chart can visualize class-wise accuracies to spot any classes that are systematically harder.","metadata":{"_uuid":"f7baa0ef-b829-41fb-8ada-1548c16e48a5","_cell_guid":"2b8e0f55-5a11-4b5d-9ea2-07956f4a0757","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"classes = [str(i) for i in range(10)]\nacc_per_class = cm.diagonal() / cm.sum(axis=1)\n\nplt.figure(figsize=(6,4))\nplt.bar(classes, acc_per_class)\nplt.ylim(0,1.0)\nplt.xlabel('Digit Class'); plt.ylabel('Accuracy')\nplt.title('Class-wise Accuracy on Validation')\nplt.show()","metadata":{"_uuid":"3c3cce20-6a58-4c61-86a6-d986c36970c4","_cell_guid":"0dfcb5d9-7e86-4608-a5a8-db04bf710262","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The class-wise accuracy plot (not shown) helps diagnose if certain digits (e.g. 1 vs. 7) are more often confused.","metadata":{"_uuid":"356d167e-1945-45fd-913a-d4d2a5c5e5ca","_cell_guid":"917e3c58-51ad-4e0b-b372-1bc8725e8051","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nTest Predictions and Submission\n</div>\n\nFinally, we generate predictions on the Kaggle test set and format them for submission. The Kaggle evaluation uses categorization accuracy (fraction of correctly labeled test images). The submission requires a CSV with columns ImageId,Label, where ImageId starts at 1.","metadata":{"_uuid":"8f5d876a-c735-4461-be9b-8fd1f5151140","_cell_guid":"c13b93e3-f5c9-40a4-b64b-fd60a13e906e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.eval()\nimage_ids = []\npredictions = []\n\nwith torch.no_grad():\n    for i, images in enumerate(test_loader):\n        images = images.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        # Compute the corresponding ImageIds for this batch\n        start_id = i * batch_size + 1\n        for offset, pred in enumerate(preds):\n            image_ids.append(start_id + offset)\n            predictions.append(pred)\n\nsubmission_df = pd.DataFrame({'ImageId': image_ids, 'Label': predictions})\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(submission_df.head())","metadata":{"_uuid":"c8566c24-606b-483b-94a5-51ccdb059b93","_cell_guid":"bfc9dc25-c8f2-45c7-a1d9-1c8fd7696ac1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We iterate through test_loader (which has no labels), predict the digit for each image, and record it.\n\nWe assemble a DataFrame with the required format and save it as submission.csv. This file can be uploaded to Kaggle for leaderboard evaluation.","metadata":{"_uuid":"54427b7e-8d71-415d-8b53-bdf8396f09d5","_cell_guid":"035e7fbf-74ae-4be2-a737-2acc774e2af8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"---\n<div style=\"border: 2px solid #444; padding: 10px; border-radius: 4px; background-color: #f5f5f5; font-size:18px; font-weight:bold; margin-bottom: 15px;\">\nExtensions and Future Work\n</div>\n\nThis notebook estatblishes a baseline foundational CNN. Future experiments could include:\n\n**Interpretability:** Techniques like saliency maps or Grad-CAM can visualize which pixels most influence the network’s predictions, helping us interpret its decision-making. This aligns with model introspection research to trust CNN predictions.\n\n**Data Augmentation & Complexity:** Further augmentations (shifts, affine transforms) or more epochs can be explored to improve generalization.\n\n**Architectural Variations:** One could try deeper networks (e.g. residual blocks) or different regularization (e.g. weight decay, early stopping).\n\n**Domain Transfer:** The trained model could be transferred to related tasks. For example, the Fashion MNIST dataset contains 28×28 grayscale images of clothing items, intended as a more challenging replacement for MNIST. Adapting the model to Fashion MNIST (with fine-tuning) is an interesting extension.\n\n**Semi-supervised Learning:** Leveraging unlabeled data or advanced losses (e.g. consistency regularization) could further boost accuracy.\n\n**Performance Analysis:** We could benchmark this model’s speed/memory or experiment on respective hardware accelerators (GPUs/TPUs) for efficiency.\n\nTo support clarity and reproducibility, all code has been written from the ground up using only essential libraries (PyTorch, torchvision, sklearn, pandas, matplotlib). The results (loss curves, confusion matrix, etc.) are fully determined by these computations and can be regenerated exactly given the fixed random seed.","metadata":{"_uuid":"84fd35f7-1f64-4044-9816-f3dafcab32d0","_cell_guid":"92845fac-8b6b-4655-80af-182e63406f44","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**Sources:** We used the standard MNIST and Kaggle descriptions, PyTorch documentation on BatchNorm and Dropout, and scikit-learn for evaluation metrics. The Fashion-MNIST reference is cited for context on dataset alternatives. These sources provide the factual and methodological backing for our implementation. We also benefited from the broader Kaggle community, whose shared notebooks and discussions provided valuable perspective and inspiration during development.","metadata":{"_uuid":"993ab1d5-59e6-4a08-b51a-9b2e9fce3539","_cell_guid":"34ff3f44-4671-4eeb-8b1b-588d5c54d818","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}